---
title: "Coursera Practical Machine Learning Project"
output_dir: "."
---
## Synopsis

The goal of this project is to use machine learning **to predict the manner in which a certain exercise was performed** using data collected in the following research:

***Qualitative Activity Recognition of Weight Lifting Exercises***

*Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.*

[Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013]( http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises)


### Componenents of a Predictor
Following J Leek's suggested [model](https://www.coursera.org/learn/practical-machine-learning/lecture/116Tb/what-is-prediction) for building a predictor we aim to:  
1. Start with a specific and well-defined **question**  
2. Gather the best **input data** to use for prediction  
3. Decide or compute **features** from the input data  
4. Apply machine learning **algorithms** (in training set)  
5. Estimate the **parameters** of the algorithms (in training set)  
6. Apply and **evaluate** these parameters (in test set)  

##1. The Question
The question we are seeking to answer is *by processing and applying algorithms to the collected observations of various accelerometers - can the quality (class A-E) of the performance of the weight-lifting exercise be predicted?*

##2. Getting and cleaning the data

The training data for this project are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


The predictor we are interested in is the “classe” variable in the training set.

```{r, warning=FALSE, message = FALSE, echo=FALSE}
library(knitr)
library(dplyr)
library(ggplot2)
library(downloader)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)

```

```{r, cache=TRUE}
# URLs of the training and testing data
  url.train  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  url.test   <-  "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

  f.train  <-  "./data/pml-training.csv"
  f.test   <-  "./data/pml-testing.csv"
# if directory does not exist, create new
  if (!file.exists("./data")) {
    dir.create("./data")
  }
# if files does not exist, download the files
  if (!file.exists(f.train)) {
    download(url.train, destfile=f.train)
  }
  if (!file.exists(f.test)) {
    download(url.test, destfile=f.test)
  }
# load the CSV files and glimpse the data 
  rawTrain  <-  read.csv(f.train, na.strings = c("NA", ""))
  rawTest   <-  read.csv(f.test, na.strings = c("NA", ""))

  dim(rawTrain)
  dim(rawTest)
# check to see that all column names are identical
  all.equal.character(colnames(rawTrain), colnames(rawTest))
  chk.cols <- colnames(rawTrain) == colnames(rawTest)
  match(FALSE, chk.cols)
  c(colnames(rawTrain[160]), colnames(rawTest[160]))

```
We identify that the two sets of data contain identical column names except the final variable in each case (in the training set it is our predictor) and in the test set it is where our prediction will be logged.

Run a check to see how many observations in the training set are not complete.

```{r}
  ok <- complete.cases(rawTrain)
  sum(!ok) # how many are not "ok" ?
    
```
`r sum(!ok)` observations are not complete - so we'll remove the columns that contains NA values.

```{r, cache=TRUE}
  c1Train <- rawTrain[, colSums(is.na(rawTrain)) == 0] 
  c1Test  <- rawTest[, colSums(is.na(rawTest)) == 0] 
  
  ok <- complete.cases(c1Train)
  sum(!ok) # how many are not "ok" ?
  
```


##3 Deciding on Features

We want to identify the features that best describe the data under examination.
From the research paper we identify that the readings were taken from sensors in the users’ glove, armband, lumbar belt and dumbbell.  The measurements were recorded for each sensor on the roll, pitch and yaw, as well as the raw accelerometer, gyroscope and magnetometer readings.  These are the variables we wish to keep and run through our algorithms.  We can safely dispose of the first seven variables which have little bearing on the outcome.

```{r, cache=TRUE}

c2Train <- c1Train[,8:length(colnames(c1Train))]
c2Test  <- c1Test[, 8:length(colnames(c1Test))]
validSet <-c2Test; rm(c2Test) # just a rename for sake of consistency later
```

This takes the number of variables down to `r sum(ncol(c2Train))` in both datasets.


##4. Applying prediction algorithms in Training
Max Kuhn describes  
* Training Set: these data are used to estimate model parameters and to pick the values of the complexity parameter(s) for the model.  
* Test Set (aka validation set): these data can be used to get an independent assessment of model efficacy.  
We'll keep our rather small `test` set to use as our 'validation' set - this is the sample where the final prediction will be performed.  
However, as we have a relatively large 'train' set, we will create data partitions (as a 'train' and 'test' set) to build and evaluate the model we decide upon.  This requires us to split up `train` - we will set seed for the purposes of reproducibility.

```{r, cache=TRUE}
set.seed(777)
inTrain  <- createDataPartition(y=c2Train$classe, p=3/4, list=FALSE)
trainSet <- c2Train[inTrain, ]
testSet  <- c2Train[-inTrain, ]

```

We now have a 25/75 split of the original data set into:  

- `testSet` with `r sum(nrow(testSet))` observations 
- `trainSet` with `r sum(nrow(trainSet))` observations 

We will use cross-validation in `trainSet` to improve the model and `testSet` will be used for our out-of-sample error rate.

###Classification Tree prediction

*A classification tree searches through each predictor to find a value of a single variable that best splits the data into two groups. Typically, the best split minimizes impurity of the outcome in the resulting data subsets.*
(Kuhn, 2013)

Here we nclude a 5-fold cross-validation.

```{r, cache=TRUE}
set.seed(777)
control.CT <- trainControl(method = "cv", number = 5)
mdl.CT <- train(classe ~ ., data = trainSet, method = "rpart", trControl = control.CT)

print(mdl.CT, digits = 4)

```

```{r}
fancyRpartPlot(mdl.CT$finalModel)
```

###Random Forests prediction

```{r, cache=TRUE}
set.seed(777)
control.RF <- trainControl(method = "cv", number = 5)
mdl.RF <- train(classe ~ ., data = trainSet, method = "rf", trControl = control.RF, ntree=250)

print(mdl.RF, digits = 4)
```
##5. Estimate the parameters of the algorithms in Training 

```{r, cache=TRUE}
predict.CT <- predict(mdl.CT, testSet)

Accuracy.CT <-confusionMatrix(predict.CT, testSet$classe)

Acc.rnd <- round(Accuracy.CT$byClass[3],3)

print(Accuracy.CT)

```

Our accuracy with Classification Tree is `r Acc.rnd`.  Meaning an out-of-sample error rate of `r 1-Acc.rnd` is not going to predict the outcome of `Classe` very well.

```{r, cache=TRUE}
predict.RF <- predict(mdl.RF, testSet)

Accuracy.RF <-confusionMatrix(predict.RF, testSet$classe)

Acc.rnd.RF <- round(Accuracy.RF$byClass[3],3)

print(Accuracy.RF)

```

Our accuracy with Random Forest is `r Acc.rnd.RF` leading to a better out-of-sample rate of `r 1-Acc.rnd.RF`.


##6. Apply and evaluate in the validation (test) set
Having selected the random forest model as our best predictor we now apply the model to our as yet untouched validation set.

```{r}

predValidation <- predict(mdl.RF, validSet)
resultsValidation <- data.frame(
  problem_id=validSet$problem_id,
  predicted=predValidation
)
print(resultsValidation)
```



#References
Kuhn, M (2013) [*Predictive Modeling with R and the caret Package*](https://www.r-project.org/nosvn/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf) viewed 2016/09/30

